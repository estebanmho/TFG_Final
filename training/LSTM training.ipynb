{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import tensorflow as tf\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "#from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"fd4vvtUF8yhmK6cfxpXuvh",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def tokenize(sentences):\n",
    "    # Create tokenizer\n",
    "    text_tokenizer = Tokenizer()\n",
    "    # Fit texts\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Lower case the sentence\n",
    "    lower_case_sent = sentence.lower()\n",
    "    # Strip punctuation\n",
    "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
    "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
    "   \n",
    "    return clean_sentence"
   ],
   "execution_count":31,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"gSOZHKtj4OdvvN01GcHPRg",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "translation_file = open(\"pruebaLSTM.csv\",\"r\", encoding='utf-8') \n",
    "raw_data = translation_file.read()\n",
    "translation_file.close()\n",
    "\n",
    "# Parse data\n",
    "raw_data = raw_data.split('\\n')\n",
    "pairs = [sentence.split(',') for sentence in  raw_data]\n",
    "pairs = pairs[:-1]"
   ],
   "execution_count":32,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"2rxoLkMh32iDSS5NAxdv5x",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "out_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
    "in_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
    "\n",
    "# Tokenize words\n",
    "in_text_tokenized, in_text_tokenizer = tokenize(in_sentences)\n",
    "out_text_tokenized, out_text_tokenizer = tokenize(out_sentences)\n",
    "\n",
    "print('Maximum length spanish sentence: {}'.format(len(max(in_text_tokenized,key=len))))\n",
    "print('Maximum length english sentence: {}'.format(len(max(out_text_tokenized,key=len))))\n",
    "\n",
    "\n",
    "# Check language length\n",
    "in_vocab = len(in_text_tokenizer.word_index) + 1\n",
    "out_vocab = len(out_text_tokenizer.word_index) + 1\n",
    "print(\"Spanish vocabulary is of {} unique words\".format(in_vocab))\n",
    "print(\"English vocabulary is of {} unique words\".format(out_vocab))"
   ],
   "execution_count":33,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Maximum length spanish sentence: 10\n",
      "Maximum length english sentence: 6\n",
      "Spanish vocabulary is of 5 unique words\n",
      "English vocabulary is of 7 unique words\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"x4QSWRVmW6koKESzy8ExyN",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "max_in_len = int(len(max(in_text_tokenized,key=len)))\n",
    "max_out_len = int(len(max(out_text_tokenized,key=len)))\n",
    "\n",
    "in_pad_sentence = pad_sequences(in_text_tokenized, max_in_len, padding = \"post\")\n",
    "out_pad_sentence = pad_sequences(out_text_tokenized, max_out_len, padding = \"post\")\n",
    "\n",
    "# Reshape data\n",
    "in_pad_sentence = in_pad_sentence.reshape(*in_pad_sentence.shape, 1)\n",
    "out_pad_sentence = out_pad_sentence.reshape(*out_pad_sentence.shape, 1)"
   ],
   "execution_count":34,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"yZbE4YsselcaYnnI8uSkzw",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "input_sequence = Input(shape=(max_in_len,))\n",
    "embedding = Embedding(input_dim=in_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_out_len)(encoder)\n",
    "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n",
    "logits = TimeDistributed(Dense(out_vocab))(decoder)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"aidSqZnICQca9wEbQIdqM3",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zX1iJ7i2FiQhUp5zhIICqH",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "model_results = enc_dec_model.fit(in_pad_sentence, out_pad_sentence, batch_size=30, epochs=1000)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"exnzgMhByZLeFjRbrOiJkB",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def logits_to_sentence(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<empty>' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "index = 60\n",
    "print(\"The english sentence is: {}\".format(out_sentences[index]))\n",
    "print(\"The spanish sentence is: {}\".format(in_sentences[index]))\n",
    "print('The predicted sentence is :')\n",
    "print(logits_to_sentence(enc_dec_model.predict(in_pad_sentence[index:index+1])[0], out_text_tokenizer))"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"rRXFecmftNOiTJCZXUiFIk",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "in_text_tokenized_p = in_text_tokenizer.texts_to_sequences(['2 2 0 0 0 0 0 2 2 2'])\n",
    "\n",
    "in_pad_sentence_p = pad_sequences(in_text_tokenized_p, max_in_len, padding = \"post\")\n",
    "\n",
    "logits_to_sentence(enc_dec_model.predict(in_pad_sentence_p[0:0+1])[0], out_text_tokenizer)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ZX9RI7A2YIdy79s4okgz12",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "# saving\n",
    "with open('in_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(in_text_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('out_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(out_text_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Data to be written \n",
    "dictionary ={ \n",
    "  \"in_max_length\": max_in_len,\n",
    "  \"out_max_length\": max_out_len\n",
    "} \n",
    "with io.open('max_length', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(dictionary, ensure_ascii=False))\n"
   ],
   "execution_count":42,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "\"with io.open('max_length', 'w', encoding='utf-8') as f:\\n    f.write(json.dumps(dictionary, ensure_ascii=False))\""
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"SJkF3mjQlDlPfosnfHZsqW",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "# loading\n",
    "with open('in_tokenizer.pickle', 'rb') as handle:\n",
    "    in_text_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('out_tokenizer.pickle', 'rb') as handle:\n",
    "    out_text_tokenizer = pickle.load(handle)"
   ],
   "execution_count":6,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Spanish vocabulary is of 5 unique words\n",
      "English vocabulary is of 7 unique words\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"sgl0JMC183ESOv4gxLJL8o",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      
     }
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}